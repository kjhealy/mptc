---
title: "Scraping and APIs"
subtitle: "Modern Plain Text Social Science<br />Week 10"
format: kjhslides-revealjs
engine: knitr
filters:
  - invert-h1
  - line-highlight
  - include-code-files
author:
  - name: Kieran Healy
    affiliation: "Duke University"
date: last-modified
editor_options:
  chunk_output_type: console
---


```{r}
#| label: "packages"
#| include: FALSE
library(flipbookr)
library(here)
library(tidyverse)
library(kjhslides)
```


```{r}
#| label: "setup"
#| include: FALSE

kjh_register_tenso()
kjh_set_knitr_opts()
kjh_set_slide_theme()

```


## Load the packages, as always

```{r}
#| label: "07-iterating-on-data-2"
#| message: TRUE
library(here)      # manage file paths
library(socviz)    # data and some useful functions
library(tidyverse) # your friend and mine
library(rvest)     # For web-scraping
```



# Scraping

## Scraping is fundamentally unclean

It is awkward

It is very prone to error

It is brittle

It's often against the terms of service of websites


## [Server-side]{.fg-orange} rendering

If the webpages you are interested in looking at are statically or dynamically assembled on the _server_ side, they arrive in your browser "fully formed". The tables or other data structures they may contain are actually populated with numbers.

The task is to get them out by identifying the HTML elements (e.g. `<table>`, `<tr>`, `<td>`, `<li>` etc) or the CSS styling selectors (e.g. `.pagetable`, `.datalist`, `#website-content-tabledata` etc) and then extract the values they enclose.

HTML elements have a fixed number of names; CSS selectors have a fixed identifying format (the `.` and `#` prefixes, etc) and internal structure, but they can be named according to the needs of the site designer.


## [Client-side]{.fg-lblue} rendering

If the webpages you are interested in looking at are assembled on the _client_ side, then what comes down is roughly in two pieces: an empty template for the the layout of the page; and a set of instructions for pulling data separately and to fill  the template.

The process of requesting specific data takes place through some private or public _application programming interface_ or API.

An API takes well-formed requests for data and returns whatever is requested in a structured format (usually JSON or XML) that can be used by the client. So we should just talk to the API directly if we can.

# Scraping Example

## Getting a single table

A single table where we know the class.

[Consider this table](https://kieranhealy.org/blog/archives/2015/02/03/another-look-at-the-california-vaccination-data/)

::: {style="font-size: 50%;"}

|             Type             |  Mean PBE  |  Median PBE  |  Max PBE  |  Min PBE  | N Schools | N Students |
|:----------------------------:|:----------:|:------------:|:---------:|----------:|----------:|-----------:|
|              Private Waldorf |   47.49    |    44.19     |   84.21   |  20       |     16    |       513  |
|            Public Montessori |   17.08    |    12.24     |   54.55   |   5.97    |     11    |       706  |
|           Charter Montessori |   14.28    |    10.26     |   31.67   |   4.35    |      5    |       227  |
|                      Charter |   10.76    |     3.03     |   70.37   |    0      |    314    |    19,863  |
|            Private Christian |    6.74    |     3.70     |   92.86   |    0      |    333    |     8,763  |
|         Private Non-Specific |    5.89    |     0        |   86.96   |    0      |    596    |    16,795  |
|           Private Montessori |    4.64    |     0        |   35.71   |    0      |     98    |     2,101  |
|    Private Jewish or Islamic |    2.59    |     0        |   14.29   |    0      |      8    |       237  |
|                       Public |    2.33    |     0.81     |   75      |    0      |   5314    |   472,802  |
|            Private Catholic  |    1.80    |     0        |   27.78   |    0      |    333    |     8,855  |
| Private Christian Montessori |    1.25    |     0        |    5      |    0      |      4    |        78  |

:::

## Getting a single table

It's on my website. It's the only table on the page, which means we can just select it with by saying we want the `<table>` element on the page. We use `html_element()` to isolate the table and `html_table()` to parse it to a tibble:

::: {.smallcode}


```{r}
vactab <- read_html("https://kieranhealy.org/blog/archives/2015/02/03/another-look-at-the-california-vaccination-data/")

vactab |>
  html_element("table") |>
  html_table() |>
  janitor::clean_names()
```

:::

## When there's more than one

Next, consider the tables [on this page](https://kieranhealy.org/blog/archives/2013/06/19/lewis-and-the-women/). Again, these are static tables, but there are several of them.

With `html_element()` we'll just extract the first one:

::: {.smallcode}


```{r}
philtabs <- read_html("https://kieranhealy.org/blog/archives/2013/06/19/lewis-and-the-women/")

philtabs |>
  html_element("table") |>
  html_table() |>
  janitor::clean_names()

```

:::

## When there's more than one

With `html_elements()` we'll extract all of them:

```{r}
philtabs |>
  html_elements("table")
```

## When there's more than one

We can get the nth one with `pluck()`:

```{r}
philtabs |>
  html_elements("table") |>
  pluck(2) |>
  html_table() |>
  janitor::clean_names()
```

Or we can use [Selector Gadget]{.fg-lblue} (or an equivalent dev tool) to find the CSS or XPath selector to the specific table, if we can find one.

## Wikipedia tables

For a long time, Wikipedia tables were fairly straightforward to select because they were static. These days many of them have a Javascript element that makes them sortable by column, but also makes them harder to grab with a CSS selector. Getting all the table elements on a page and cleaning later is often the path of least resistance.

## Wikipedia tables

```{r}
irl_demog <- read_html("https://en.wikipedia.org/wiki/Demographics_of_the_Republic_of_Ireland")

irl_demog |>
  # Get all the tables classed as `.wikitable` on the page
  html_elements(".wikitable") |>
  pluck(7) |>
  html_table() |>
  janitor::clean_names()
```


## Again, scraping is [unclean]{.fg-orange}

Scraping can be useful to quickly grab a table or two that you need from a website. For harvesting large amounts of data it is no longer a very good idea, on the whole, and may get you banned from websites if you abuse it.


# APIs

## The idea of an API

[Zapier](https://zapier.com) provide a nice overview of APIs in a [web guide](https://zapier.com/resources/guides/apis) that you can work your way through or skim.

We use APIs when requesting data directly. An API has a restricted set of protocols and methods by which a client (which can be you, but also can be your browser or an application) can ask it for data.

It has a defined set of responses (to say "OK" or "No" or "That didn't work") and formats in which it provides the client with an answer. Usually this will be a blob of JSON or XML data.

## API endpoints

In a similar way that a URL specifies a request for a specific webpage, an API endpoint is a URL-like request for a specific blob of data. The trick is specifying the correct URL, which in essence specifies a request to API.


# Example: the NY Citibike API

## NYC CitiBikes

Many cities have Bike Share programs. Many of those provide data according to the [GBFS specification](https://github.com/MobilityData/gbfs/blob/master/gbfs.md). A spec like this is a set of rules that says "If you adhere to this spec, you will provide data in the following consistent way", where this includes rules about the data format and the endpoints or URLs where that data can be found.

## NYC CitiBikes

The spec has a rule saying "Provide a feed specifying the data available".

::: {.smallcode}
```{r}
gbfsurl <- "https://gbfs.citibikenyc.com/gbfs/2.3/gbfs.json"
feeds <- jsonlite::fromJSON(gbfsurl)
str(feeds)
```

This comes in to us a _nested list_.

:::

## NYC CitiBikes

JSON data is typically nested. It can look really complex at first glance. This is one reason to read the spec. We can put the feed into a tibble if we like:

```{r}
feeds_df <- as_tibble(feeds)
feeds_df
```

RStudio's object viewer is a good way to explore the hierarchical structure of unfamiliar lists.

## NYC CitiBikes

If we explore or look at the spec we see that each row provides the same feed information in English, Spanish, or French. We can slice out the English feed and look at what's in it:

```{r}
feeds_df |>
  slice(1) |>
  unnest(data) |> # It's two levels down
  unnest(data)
```

More urls! Let's extract the station data feed.

## NYC CitiBikes

```{r}
nyc_stations_url <- feeds_df |>
  slice(1) |> unnest(data) |> unnest(data) |>
  filter(name == "station_information") |> pull(url)

nyc_stations_url
```

## NYC CitiBikes

That was tedious. If we know _precisely_ what we're after we can do it faster by plumbing down through the list:

```{r}
# Base R style
feeds_df[[1]]$en$feeds$url[3]
```

Or with `pluck()`:

```{r}
# Pluck by element number or name
feeds_df |> pluck(1,"en", "feeds", "url", 3)
```

## NYC CitiBikes

Or with some combination of methods:

```{r}
feeds_df |>
  pluck(1,"en", "feeds") |>
  as_tibble()
```



Then work as you would with a tibble.

## NYC CitiBikes

```{r}
station_status_url <- feeds_df |> pluck(1,"en", "feeds") |>
  filter(name == "station_status") |> pull(url)

station_status_df <- jsonlite::fromJSON(station_status_url)

str(station_status_df) # Still a list
```

Like the overall feeds data, the station status data is a nested list with information on when it was last updated and the software version as well as the actual data, which is in `station_df$data$stations`. We can get it out.

## NYC CitiBikes

::: {.smallcode}
```{r}
station_status_df <- station_status_df$data$stations |>
  as_tibble()

station_status_df
```

:::


As you can see there is more nested data in here too, in the column about the types of bike and scooter models available.

## NYC CitiBikes

Let's do the same for the feed of static information about stations.

```{r}
station_info_url <- feeds_df |> pluck(1,"en", "feeds") |>
  filter(name == "station_information") |> pull(url)

station_info_df <- station_info_url |>
  jsonlite::fromJSON() |>
  pluck("data", "stations") |>
  as_tibble()

station_info_df

```

## NYC CityBikes

We can join these by `station_id`

```{r}
stations_df <- station_status_df |>
  left_join(station_info_df, by = "station_id") |>
  relocate(name, capacity, num_bikes_available, lat, lon)

stations_df
```


## NYC CityBikes

::: {.smallcode}

```{r}
#| output-location: column
#| classes: custom3070

stations_df |>
  ggplot(aes(x = lon,
             y = lat,
             color = num_bikes_available/capacity)) +
  geom_point(size = 0.5) +
  scale_color_viridis_c(option = "plasma",
                        labels = scales::percent_format()) +
  coord_equal() +
  labs(color = "Availability",
       title = "New York CitiBike Stations",
       subtitle = "Current bike availability as a percentage of station capacity") +
  theme_void()
```

:::

## NYC CitiBikes

Finally, the "last updated" tag is a good old Unix time number expressed in seconds since 1970:

```{r}
feeds_df$last_updated[1]
```

```{r}
as_datetime(feeds_df$last_updated)[1]
```


# Hidden APIs

## Hidden APIs

Remember, dynamic or client-side content has to come from somewhere.
