---
title: "Iterating on<br />Data and Models"
subtitle: "Modern Plain Text Social Science<br />Week 09"
format: kjhslides-revealjs
engine: knitr
filters:
  - invert-h1
  - include-code-files
  - line-highlight
author:
  - name: Kieran Healy
    email: kieran.healy@duke.edu
date: last-modified
---


# [Iterating]{.fg-green} on data with [purrr]{.fg-yellow}

## Load the packages, as always

```{r}
#| label: "07-iterating-on-data-2"
#| message: TRUE
library(here)      # manage file paths
library(socviz)    # data and some useful functions
library(tidyverse) # your friend and mine
```

---

::: {.huge .center}

[**Split**]{.fg-lblue}

[**Apply**]{.fg-red}

[**Combine**]{.fg-green}

:::


---

## The Core Idea

- [Split]{.fg-lblue} or group the data into pieces
- [Apply]{.fg-red} a function, i.e. act on the data in some way
- [Combine]{.fg-green} the results back into a table

::::: {.fragment fragment-index=2}
Sometimes we start out in a "split" state, as when we have multiple data files, and we need to read them in and then combine them.
:::::


## More than one data file

- Inside `files/examples/` is a folder named `congress/`

::: {.smallcode}

```{r }
#| label: "07-iterating-on-data-3"
# A little trick from the fs package:
fs::dir_tree(here("files", "examples", "congress"))
```

:::

## More than one data file

Let's look at one.

::: {.smallcode}

```{r }
#| label: "07-iterating-on-data-4"
read_csv(here("files", "examples", "congress", "17_95_congress.csv")) |>
  janitor::clean_names() |>
  head()
```

:::

We often find ourselves in this situation. We know each file has the same structure, and we would like to use them all at once.

## Loops?

How to read them all in?

One traditional way, which we could do in R, is to write an explicit _loop_ that iterated over a vector of filenames, read each file, and then stack the results all together in a tall rectangle.

```r
## Pseudocode (i.e. will not really run)
## Also, if you do write loops, do not use them to grow dataframes in this way.

filenames <- c("01_79_congress.csv", "02_80_congress.csv", "03_81_congress.csv",
                "04_82_congress.csv" [etc etc])

collected_files <- NULL

for(i in 1:length(filenames)) {
      new_file <- read_file(filenames[i])
      collected_files <- append_to(collected_files, new_files)
}


```

## Loops?

- You may have noticed we have not written any loops, however.
- While loops are still lurking there underneath the surface, what we will do instead is to take advantage of the combination of vectors and functions and _map_ one to the other in order to generate results.
- Speaking loosely, think of [**`map()`**]{.fg-green} as a way of [iterating]{.fg-orange} without explicitly writing loops. You start with a vector of things. You feed it one thing at a time to some function. The function does whatever it does. You get back output that is the same length as your input, and of a specific type.


## Mapping is just a kind of iteration

::: {.incremental}
- The `purrr` package provides a big family of mapping functions. One reason there are a lot of them is that `purrr`, like the rest of the tidyverse, is picky about data types.
- So in addition to the basic [**`map()`**]{.fg-green}, which always returns a _list_, we also have [**`map_chr()`**]{.fg-green}, [**`map_int()`**]{.fg-green}, [**`map_dbl()`**]{.fg-green}, [**`map_lgl()`**]{.fg-green} and others. They always return the data type indicated by their suffix, or die trying.
:::

## Vectorized arithmetic again

The simplest cases are not that different from the vectorized arithmetic we're already familiar with.

```{r }
#| label: "07-iterating-on-data-5"
a <- c(1:10)

b <- 1

# You know what R will do here
a + b

```

::::: {.fragment fragment-index=1}
R's vectorized rules add `b` to every element of `a`. In a sense, the [**`+`**]{.fg-green} operation can be thought of as a function that takes each element of `a` and does something with it. In this case "add `b`".
:::::


## Vectorized arithmetic again

We can make this explicit by writing a function:

```{r }
#| label: "07-iterating-on-data-6"
add_b <- function(x) {
  b <- 1
  x + b # for any x
}
```


## Vectorized arithmetic again

We can make this explicit by writing a function:

```{r }
#| label: "07-iterating-on-data-6b"
add_b <- function(x) {
  b <- 1
  x + b # for any x
}
```


Now:

```{r }
#| label: "07-iterating-on-data-7"
add_b(x = a)
```

## Vectorized arithmetic again

Again, R's vectorized approach means it automatically adds `b` to every element of the x we give it.

```{r }
#| label: "07-iterating-on-data-8"
add_b(x = 10)
```

```{r }
#| label: "07-iterating-on-data-9"
add_b(x = c(1, 99, 1000))
```

## [Iterating]{.fg-green} in a pipeline

Some operations can't directly be vectorized in this way, which is why we need to manually iterate, which will make us want to write loops.

```{r}
library(gapminder)

# Not what we want in this case
gapminder |> n_distinct()
```

When you find yourself saying "No, I meant *for each* ..."

```{r }
#| label: "07-iterating-on-data-10"

gapminder |>
  summarize(country_n = n_distinct(country),
            continent_n = n_distinct(continent),
            year_n = n_distinct(year),
            lifeExp_n = n_distinct(lifeExp),
            population_n = n_distinct(population))
```

That's tedious to write! Computers are meant to let us avoid this sort of thing.

## [Iterating]{.fg-green} in a pipeline

So how would we iterate this? What we want is to apply the [**`n_distinct()`**]{.fg-green} function to each column of `gapminder`, but in a way that still allows us to use pipelines and so on.

```{r }
#| label: "07-iterating-on-data-11"
library(gapminder)
gapminder |>
  summarize(n_distinct(country),
            n_distinct(continent),
            n_distinct(year),
            n_distinct(lifeExp),
            n_distinct(population))
```

::: aside
Using [**`n_distinct()`**]{.fg-green} in this context is an idea I got from Rebecca Barter's discussion of `purrr`.
:::

## [Iterating]{.fg-green} in a pipeline

You'd use [**across()**]{.fg-green}, like this:

```{r }
#| label: "07-iterating-on-data-12"
gapminder |>
  summarize(across(everything(), n_distinct))
```

In this context, the function is applied across the specified columns and you're always getting a tibble back.

## [Iterating]{.fg-green} in a pipeline

But you could also do this ...

:::: {.columns}
::: {.column width="50%"}
```{r }
#| label: "07-iterating-on-data-13"
  map(gapminder, n_distinct)
```

:::

::: {.column width="50%" .right}
- Read it as "Feed each column of `gapminder` to the [**`n_distinct()`**]{.fg-green} function.
- (This is pretty much what [**`across()`**]{.fg-green} is doing more nicely.)
:::
::::

## [Iterating]{.fg-green} in a pipeline

:::: {.columns}
::: {.column width="50%"}
Or, in pipeline form:

```{r }
#| label: "07-iterating-on-data-14"
gapminder |>
  map(n_distinct)
```

:::

::: {.column width="50%" .right}

You can see we are getting a _list_ back.

:::
::::


---

## [Iterating]{.fg-green} in a pipeline

Or, in pipeline form:

```{r }
#| label: "07-iterating-on-data-15"
result <- gapminder |>
  map(n_distinct)

class(result)

result$continent

result[[2]]
```


## [Iterating]{.fg-green} in a pipeline

But we know [**`n_distinct()`**]{.fg-green} should always return an integer. We just want a vector of integers back, not a list of them. So we use [**`map_int()`**]{.fg-green} instead of the generic [**`map()`**]{.fg-green}.


```{r }
#| label: "07-iterating-on-data-16"
gapminder |>
  map_int(n_distinct)
```

The [**`map()`**]{.fg-green} family can deal with all kinds of input types and output types. We choose some specific variant of `map` based on two things: what (and how many) things we are feeding the function we're mapping, and what type of thing it should return.



## Get a vector of [filenames]{.fg-pink}

```{r }
#| label: "07-iterating-on-data-17"
filenames <- dir(path = here("files", "examples", "congress"),
                 pattern = "*.csv",
                 full.names = TRUE)

filenames[1:15] # Just displaying the first 15, to save slide space

```

## And feed it to [`read_csv()`]{.fg-green}

... using [**`map()`**]{.fg-green} and binding the resulting list into a tibble.

```{r }
#| label: "07-iterating-on-data-18"
df <- filenames |>
  map(read_csv) |> #<<
  list_rbind(names_to = "congress") |>
  janitor::clean_names()

df
```

---

![](../assets/08-iterate/emperor-witness.png)

## [`read_csv()`]{.fg-green} can do this directly

In fact `map()` is not required for this particular use:

```{r }
#| label: "07-iterating-on-data-19"
tmp <- read_csv(filenames, id = "path",
                name_repair = janitor::make_clean_names)

tmp |>
  mutate(congress = stringr::str_extract(path, "_\\d{2,3}_congress"),
         congress = stringr::str_extract(congress, "\\d{2,3}")) |>
  relocate(congress)

```


# Wrangling [Models]{.fg-green}

## This is not a [statistics]{.fg-yellow} seminar!

- I'll just give you an example of the sort of thing that many other modeling packages implement for all kinds of modeling techniques.
- Again, the principle is tidy incorporation of models and their output.
- Which means, as always: tibbles. We want a _data frame_ of our results. A nice rectangular table that we know what to do with regardless of what its contents mean.

## Tidy regression output with [broom]{.fg-yellow}

```{r }
#| label: "07-iterating-on-data-61"
library(broom)
library(gapminder)
```


```{r}
#| label: "r 07-[Iterating]{.fg-green}-2"
out <- lm(formula = lifeExp ~ gdpPercap + log(pop) + continent,
          data = gapminder)
```

## Tidy regression output with [broom]{.fg-yellow}

We can't _do_ anything with this, programatically.

```{r}
#| label: "r 07-[Iterating]{.fg-green}-3"
summary(out)
```

# [Look inside]{.fg-lblue} [the box]{.fg-yellow}


## Objects are To-Do List Bento Boxes


```{r }
#| label: "06-work-with-models-5"
gapminder
```

## Fit a model

```{r }
#| label: "06-work-with-models-6"
out <- lm(formula = lifeExp ~ gdpPercap + log(pop) + continent,
          data = gapminder)

summary(out)
```

## Poke around inside

![Use the Object Inspector to take a look](../assets/09-broom/06_lm_object_schematic.png)


## Tidy regression output with [broom]{.fg-yellow}

```{r}
#| label: "r 07-[Iterating]{.fg-green}-8"
library(broom)
```

```{r}
#| label: "r 07-[Iterating]{.fg-green}-9"
tidy(out)
```

That's a _lot_ nicer. Now it's just a tibble. We know those.

## Tidy regression output with [broom]{.fg-yellow}

```{r}
#| label: "r 07-[Iterating]{.fg-green}-11"
out_conf <- tidy(out, conf.int = TRUE)
out_conf
```

## Tidy regression output with [broom]{.fg-yellow}

```{r }
#| label: "07-iterating-on-data-62"
out_conf |>
    filter(term %nin% "(Intercept)") |>
    mutate(nicelabs = prefix_strip(term, "continent")) |>
    select(nicelabs, everything())
```

## Three ways to tidy

::: {.large}
- [Component level]{.fg-lblue}: [`tidy()`]{.fg-green}
:::

## Three ways to tidy

::: {.large}
- [Component level]{.fg-lblue}: [`tidy()`]{.fg-green}
- [Observation level]{.fg-orange}: [`augment()`]{.fg-green}
:::

## Three ways to tidy

::: {.large}
- [Component level]{.fg-lblue}: [`tidy()`]{.fg-green}
- [Observation level]{.fg-orange}: [`augment()`]{.fg-green}
- [Model level]{.fg-pink}: [`glance()`]{.fg-green}
:::

## Model output again

```r
> summary(out)
```

```text
Call:
lm(formula = lifeExp ~ gdpPercap + log(pop) + continent, data = gapminder)

Residuals:
    Min      1Q  Median      3Q     Max
-47.490  -4.614   0.250   5.293  26.094
```

```text
Coefficients:
                   Estimate Std. Error t value Pr(>|t|)
(Intercept)       3.816e+01  2.050e+00  18.618  < 2e-16 ***
gdpPercap         4.557e-04  2.345e-05  19.435  < 2e-16 ***
log(pop)          6.394e-01  1.329e-01   4.810 1.64e-06 ***
continentAmericas 1.308e+01  6.063e-01  21.579  < 2e-16 ***
continentAsia     7.784e+00  5.810e-01  13.398  < 2e-16 ***
continentEurope   1.695e+01  6.350e-01  26.691  < 2e-16 ***
continentOceania  1.764e+01  1.779e+00   9.916  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 8.336 on 1697 degrees of freedom
Multiple R-squared:  0.585,	Adjusted R-squared:  0.5835
F-statistic: 398.7 on 6 and 1697 DF,  p-value: < 2.2e-16
```

## [Component]{.fg-lblue} level

```r
> summary(out)
```

```text
Call:
lm(formula = lifeExp ~ gdpPercap + log(pop) + continent, data = gapminder)

Residuals:
    Min      1Q  Median      3Q     Max
-47.490  -4.614   0.250   5.293  26.094
```

```R
Coefficients:
                   Estimate Std. Error t value Pr(>|t|)
(Intercept)       3.816e+01  2.050e+00  18.618  < 2e-16 ***
gdpPercap         4.557e-04  2.345e-05  19.435  < 2e-16 ***
log(pop)          6.394e-01  1.329e-01   4.810 1.64e-06 ***
continentAmericas 1.308e+01  6.063e-01  21.579  < 2e-16 ***
continentAsia     7.784e+00  5.810e-01  13.398  < 2e-16 ***
continentEurope   1.695e+01  6.350e-01  26.691  < 2e-16 ***
continentOceania  1.764e+01  1.779e+00   9.916  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

```text
Residual standard error: 8.336 on 1697 degrees of freedom
Multiple R-squared:  0.585,	Adjusted R-squared:  0.5835
F-statistic: 398.7 on 6 and 1697 DF,  p-value: < 2.2e-16
```

## [Component]{.fg-lblue} level

```{r}
#| label: "r 07-[Iterating]{.fg-green}-9again"
tidy(out)
```


## [Observation]{.fg-orange} level

```r
> summary(out)
```

```text
Call:
lm(formula = lifeExp ~ gdpPercap + log(pop) + continent, data = gapminder)
```

```R
Residuals:
    Min      1Q  Median      3Q     Max
-47.490  -4.614   0.250   5.293  26.094
```


```text
Coefficients:
                   Estimate Std. Error t value Pr(>|t|)
(Intercept)       3.816e+01  2.050e+00  18.618  < 2e-16 ***
gdpPercap         4.557e-04  2.345e-05  19.435  < 2e-16 ***
log(pop)          6.394e-01  1.329e-01   4.810 1.64e-06 ***
continentAmericas 1.308e+01  6.063e-01  21.579  < 2e-16 ***
continentAsia     7.784e+00  5.810e-01  13.398  < 2e-16 ***
continentEurope   1.695e+01  6.350e-01  26.691  < 2e-16 ***
continentOceania  1.764e+01  1.779e+00   9.916  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```


```text
Residual standard error: 8.336 on 1697 degrees of freedom
Multiple R-squared:  0.585,	Adjusted R-squared:  0.5835
F-statistic: 398.7 on 6 and 1697 DF,  p-value: < 2.2e-16
```


## [Observation]{.fg-orange} level

```{r }
#| label: "06-work-with-models-18"
augment(out)
```

## [Observation]{.fg-orange} level

- For OLS models:

- [.fitted]{.fg-orange} — The fitted values of the model.
- [.se.fit]{.fg-orange} — The standard errors of the fitted values.
- [.resid]{.fg-orange} — The residuals.
- [.hat]{.fg-orange} — The diagonal of the hat matrix.
- [.sigma]{.fg-orange} — An estimate of the residual standard deviation when the corresponding observation is dropped from the model.
- [.cooksd]{.fg-orange} — Cook’s distance, a common regression diagnostic.
- [.std.resid]{.fg-orange} — The standardized residuals.


## [Observation]{.fg-orange} level

```{r }
#| label: "06-work-with-models-19"
# Adding the data argument puts back any additional columns from the original
# tibble
out_aug <-  augment(out, data = gapminder)
head(out_aug)

```

```{r }
#| label: "06-work-with-models-20"
## Residuals vs Fitted Values
p <- ggplot(data = out_aug,
            mapping = aes(x = .fitted, y = .resid))
p_out <- p + geom_point()
```

---

```{r}
#| label: "06-work-with-models-21"
#| echo: FALSE
#| fig.width: 15
#| fig.height: 8
p_out
```


(I told you it was misspecified)

## [Model]{.fg-pink} level

```r
> summary(out)
```

```text
Call:
lm(formula = lifeExp ~ gdpPercap + log(pop) + continent, data = gapminder)

Residuals:
    Min      1Q  Median      3Q     Max
-47.490  -4.614   0.250   5.293  26.094
```

```text
Coefficients:
                   Estimate Std. Error t value Pr(>|t|)
(Intercept)       3.816e+01  2.050e+00  18.618  < 2e-16 ***
gdpPercap         4.557e-04  2.345e-05  19.435  < 2e-16 ***
log(pop)          6.394e-01  1.329e-01   4.810 1.64e-06 ***
continentAmericas 1.308e+01  6.063e-01  21.579  < 2e-16 ***
continentAsia     7.784e+00  5.810e-01  13.398  < 2e-16 ***
continentEurope   1.695e+01  6.350e-01  26.691  < 2e-16 ***
continentOceania  1.764e+01  1.779e+00   9.916  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

```R
Residual standard error: 8.336 on 1697 degrees of freedom
Multiple R-squared:  0.585,	Adjusted R-squared:  0.5835
F-statistic: 398.7 on 6 and 1697 DF,  p-value: < 2.2e-16
```

## [Model]{.fg-pink} level


```{r }
#| label: "06-work-with-models-22"
glance(out)
```

The usefulness of [`glance()`]{.fg-green} becomes clearer when dealing with ensembles of models.

# Grouped Analysis

## Grouped analysis and [list columns]{.fg-orange}

European countries in 1977:

```{r}
#| label: "r-07-eu"
gapminder |>
    filter(continent == "Europe", year == 1977)
```

Let's fit a simple model predicting life expectancy from GDP per capita for just these countries.

## Grouped analysis and [list columns]{.fg-orange}

```{r}
#| label: "r 07-[Iterating]{.fg-green}-20"
eu77 <- gapminder |> filter(continent == "Europe", year == 1977)
fit <- lm(lifeExp ~ log(gdpPercap), data = eu77)
```


```{r}
#| label: "r 07-[Iterating]{.fg-green}-21"

summary(fit)
```

What if we want to do this for every continent in every year?

## Grouped analysis and [list columns]{.fg-orange}

```{r}
#| label: "r 07-[Iterating]{.fg-green}-22"

df_nest<- gapminder |>
    group_by(continent, year) |>
    nest()

df_nest

```

Think of nesting as a kind of "super-grouping". Look in the object inspector.

## Grouped analysis and [list columns]{.fg-orange}

It's still in there.

```{r}
#| label: "r 07-[Iterating]{.fg-green}-23"
df_nest|> filter(continent == "Europe" & year == 1977) |>
    unnest(cols = c(data))
```

## Grouped analysis and [list columns]{.fg-orange}

```{r}
#| label: "r 07-[Iterating]{.fg-green}-24"
#| echo: FALSE
old_digits <- getOption("digits")
options(digits = 3)
```

Here we [**`map()`**]{.fg-green} a custom function to every row in the `data` column.

```{r}
#| label: "r 07-[Iterating]{.fg-green}-25"

fit_ols <- function(df) {
    lm(lifeExp ~ log(gdpPercap), data = df)
}

df_nest<- gapminder |>
    group_by(continent, year) |>
    nest() |>
    mutate(model = map(data, fit_ols)) #<<
```

## Grouped analysis and [list columns]{.fg-orange}


```{r }
#| label: "07-iterating-on-data-63"
df_nest
```


## Grouped analysis and [list columns]{.fg-orange}

We can tidy the nested models, too.

```{r}
#| label: "r 07-[Iterating]{.fg-green}-26"

fit_ols <- function(df) {
    lm(lifeExp ~ log(gdpPercap), data = df)
}

out_tidy <- gapminder |>
    group_by(continent, year) |>
    nest() |>
    mutate(model = map(data, fit_ols),
           tidied = map(model, tidy)) |>
    unnest(cols = c(tidied)) |>
    filter(term %nin% "(Intercept)" &
           continent %nin% "Oceania")
```


## Grouped analysis and [list columns]{.fg-orange}

```{r }
#| label: "07-iterating-on-data-64"
out_tidy
```

## Grouped analysis and [list columns]{.fg-orange}

```{r }
#| label: "07-iterating-on-data-65"
out_tidy |>
    ungroup() |>
    sample_n(5)
```


# Join us now for the <br />![](../assets/08-iterate/kh-popup.png){width=100px style="vertical-align: middle;"} Crossover Event ![](../assets/08-iterate/sv-popup.png){width=100px style="vertical-align: middle;"}<br /> of the Year

## Bootstrapped ATT estimates

```{r}
library(WeightIt)
library(MatchIt)
data("lalonde", package = "MatchIt")
lalonde
```

I heard you were working on these in Steve's class.

## Bootstrapped ATT estimates

```{r}
n_replicates <- 1e3
set.seed(22102026)

m_orig <- matchit(treat ~ age + educ + race + married + nodegree + re74 + re75,
    data = lalonde, method = "nearest",
    estimand = "ATT")

dr_orig <- lm(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75,
    data = lalonde, weights = m_orig$weights)

dr_tidy <- tidy(dr_orig) |>
    filter(term == "treat")
dr_tidy
```

## Bootstrapped ATT estimates

Tidily bootstrap our estimates.

```{r}
## Keep everything, do the matching and weighting inside the boot
boot_out <- tibble(boot_id = 1:n_replicates) |>
  mutate(
    boot_data = map(boot_id, \(x) slice_sample(lalonde, #<<
                                               n = nrow(lalonde),
                                               replace = TRUE)),
    match_obj = map(boot_data, \(bd) {  #<<
      matchit(treat ~ age + educ + race + married + nodegree + re74 + re75,
              data = bd,
              method = "nearest",
              estimand = "ATT")
      }),
    model = map2(boot_data, match_obj, \(bd, mo) { #<<
      lm(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75,
         data = bd,
         weights = mo$weights)
      }),
    coefs = map(model, tidy) #<<
  )

```

Four `map()` calls including one to `map2()`. We could separately write named functions for the steps inside the mapping calls, but we use lambdas instead and write them on the fly.

## Bootstrapped ATT estimates

What we get:

```{r}
boot_out
```

## Bootstrapped ATT estimates

Unnest the coefs column:

```{r}
boot_out |>
  unnest(coefs)
```

## Bootstrapped ATT estimates

```{r}
# Simplest way to calculate bootstrap ci's; could get
# much fancier using boot() or any number of other fns for the
# bootstrap stage
boot_tbl <- boot_out |>
  unnest(coefs) |>
  filter(term == "treat") |>
  summarize(
    boot_att = mean(estimate),
    boot_se = sd(estimate),
    boot_ci_lower = quantile(estimate, 0.025),
    boot_ci_upper = quantile(estimate, 0.975)
  )

tinytable::tt(boot_tbl, digits = 3)
```

## Bootstrapped ATT estimates

```{r}
#| fig-cap: Distribution of Treatment Estimates
#| fig-width: 8
#| fig-height: 6
#| output-location: slide

tmp_tbl <- boot_tbl |>
  mutate(y = 0.1)

boot_out |>
  unnest(coefs) |>
  select(boot_id, term, estimate) |>
  filter(term == "treat") |>
  ggplot(mapping = aes(x = estimate, y = after_stat(scaled),
        color = term, fill = term)) +
  geom_vline(xintercept = 0, color = "gray30") +
  geom_density(alpha = 0.5) +
  geom_pointrange(data = tmp_tbl,
    mapping = aes(x = boot_att, y = y, xmin = boot_ci_lower,
        xmax = boot_ci_upper),
    color = "gray20", size = rel(0.7), linewidth = rel(0.9),
    inherit.aes = FALSE) +
  labs(title = "Distribution of Bootstrapped Treatment Estimates",
       subtitle = "Pointrange shows point estimate and 95% quantile range.",
       x = "Bootstrap Estimate", y = "Scaled Density") +
  guides(color = "none", fill = "none")
```


# Midwest PCA Example

## Midwest data

```{r}
midwest
```

## Extract numeric vars

```{r}
mw_pca <- midwest |>
    group_by(state) |>
    select(where(is.numeric)) |>
    select(-PID)

mw_pca
```

## PCA Helper function

We could do this anonymously, with a lambda function, too.

```{r}
do_pca <- function(df){
  prcomp(df,
         center = TRUE, scale = TRUE)
}
```

## PCA on the whole dataset

```{r}
out_pca <- mw_pca |>
    ungroup() |>
    select(-state) |>
    do_pca()

out_pca
```

## Tidier view, whole dataset

```{r}
tidy_pca <- tidy(out_pca, matrix = "pcs")

tidy_pca

```

## Using `augment()`

This puts the original data points back in.

```{r}
aug_pca <- augment(out_pca, data = mw_pca[,-1])
aug_pca <-  aug_pca |>
  tibble::add_column(midwest$state, midwest$county, .before = TRUE) |>
  rename(state = `midwest$state`, county = `midwest$county`)

aug_pca
```

## Grouped PCA

Let's do that grouped by State

```{r}
mw_pca <- mw_pca |>
    group_by(state) |>
    nest()

mw_pca
```

## Grouped PCA


```{r}
state_pca <- mw_pca |>
    mutate(pca = map(data, do_pca))

state_pca

```

## Grouped PCA

Alternatively, write a lambda function:

```{r}
mw_pca |>
    mutate(pca = map(data, \(x) prcomp(x, center = TRUE, scale = TRUE)))
```


## Tidy it

```{r}
# broom has methods for different models, including prcomp, and they take
# arguments specific to those models. Here we want the principal components.
do_tidy <- function(pr){
    broom::tidy(pr, matrix = "pcs")
}

state_pca  <- mw_pca |>
    mutate(pca = map(data, do_pca),
           pcs = map(pca, do_tidy))

state_pca
```

## Tidy it

Or again, write a lambda function

```{r}
mw_pca |>
    mutate(pca = map(data, \(x) prcomp(x, center = TRUE, scale = TRUE)),
           pcs = map(pca, \(x) tidy(x, matrix = "pcs")))
```


## Unnest to inspect

::: {.smallcode}

```{r}
mw_pca |>
  mutate(pca = map(data, \(x) prcomp(x, center = TRUE, scale = TRUE)),
           pcs = map(pca, \(x) tidy(x, matrix = "pcs"))) |>
  unnest(cols = c(pcs)) |>
  ggplot(aes(x = PC, y = percent)) +
  geom_line(linewidth = 1.1) +
  facet_wrap(~ state, nrow = 1) +
  labs(x = "Principal Component",
       y = "Variance Explained")
```

:::

## And finally, `augment()`

```{r}
do_aug <- function(pr){
    broom::augment(pr)
}


state_pca  <- mw_pca |>
    mutate(pca = map(data, do_pca),
           pcs = map(pca, do_tidy),
           fitted = map(pca, do_aug))

state_pca
```


## ... or a lambda function

```{r}
mw_pca |>
  mutate(pca = map(data, \(x) prcomp(x, center = TRUE, scale = TRUE)),
         pcs = map(pca, \(x) tidy(x, matrix = "pcs")),
         fitted = map(pca, \(x) augment(x)))
```


## In one breath

```{r}
out <- midwest |>
    group_by(state) |>
    select_if(is.numeric) |>
    select(-PID) |>
    nest() |>
    mutate(pca = map(data, \(x) prcomp(x, center = TRUE, scale = TRUE)),
           pcs = map(pca, \(x) tidy(x, matrix = "pcs")),
           fitted = map(pca, \(x) augment(x))) |>
    unnest(cols = c(fitted)) |>
    add_column(county = midwest$county)

out
```

---

```{r}
#| echo: false
#| message: false
#| fig-width: 12
#| fig-height: 7

ggplot(data = out, aes(x = .fittedPC1,
               y = .fittedPC2,
               label = county)) +
    geom_text(size = 1.1) +
    labs(x = "First Principal Component",
         y = "Second Principal Component") +
    theme_minimal() + facet_wrap(~ state, ncol = 2)

```
